{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec3e7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LinearRegression\n",
    "# Examples\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# y = 1 * x_0 + 2 * x_1 + 3\n",
    "\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X, y)\n",
    "reg.coef_       # slope (estimated)\n",
    "reg.intercept_  # y intercept (estimated)\n",
    "reg.predict(np.array([[3, 5]]))\n",
    "# This predicts the y value for a new input [3, 5]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This code performs Linear Regression from scratch using numpy.\n",
    "The dot fucntion in numpy does matrix multiplication or sum of products. \n",
    "\n",
    "Here, X is the input data, also called features.\n",
    "Each row is a data points: [x0, x1]\n",
    "So, there are 4 examples and each has 2 features\n",
    "\n",
    "'y = np.dot(X, np.array([1, 2])) + 3'\n",
    "    - np.array([1, 2]) represents the weights for the two features: \n",
    "        --> x0 is multiplied by 1, and x1 is multiplied by 2\n",
    "    - np.dot() performs a dot product of each row of X with [1, 2]\n",
    "    \n",
    "What np.dot() does here:\n",
    "    - it computes the sum of the products of corresponding elements.\n",
    "    - Example for first row [1, 1]:\n",
    "        1*1 + 1*2 = 1 + 2 = 3\n",
    "    \n",
    "So:\n",
    "    y = [ (1*1 + 1*2) + 3,\n",
    "      (1*1 + 2*2) + 3,\n",
    "      (2*1 + 2*2) + 3,\n",
    "      (2*1 + 3*2) + 3 ]\n",
    "  = [6, 8, 9, 11]\n",
    "\n",
    "So y becomes:\n",
    "    y = np.array([6, 8, 9, 11])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b093bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02c2921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.862\n",
      "Model:                            OLS   Adj. R-squared:                  0.806\n",
      "Method:                 Least Squares   F-statistic:                     15.56\n",
      "Date:                Tue, 22 Apr 2025   Prob (F-statistic):            0.00713\n",
      "Time:                        16:39:43   Log-Likelihood:                -24.316\n",
      "No. Observations:                   8   AIC:                             54.63\n",
      "Df Residuals:                       5   BIC:                             54.87\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          5.5226      4.431      1.246      0.268      -5.867      16.912\n",
      "x1             0.4471      0.285      1.567      0.178      -0.286       1.180\n",
      "x2             0.2550      0.453      0.563      0.598      -0.910       1.420\n",
      "==============================================================================\n",
      "Omnibus:                        0.561   Durbin-Watson:                   3.268\n",
      "Prob(Omnibus):                  0.755   Jarque-Bera (JB):                0.534\n",
      "Skew:                           0.380   Prob(JB):                        0.766\n",
      "Kurtosis:                       1.987   Cond. No.                         80.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "r2 score: 0.8615939258756776\n",
      "r2 score adjusted: 0.8062314962259487\n",
      "regression coafficients: [5.52257928 0.44706965 0.25502548]\n",
      "Predicted response: [ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n",
      "Predicted response: [ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19787\\anaconda3\\envs\\myenv\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Advanced Linear Regression With statsmodels TUTORIAL starts now\n",
    "# STEP 1: Import packages\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# STEP 2: Provide data and transform inputs\n",
    "x = np.array([\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "])\n",
    "y = np.array([4, 5, 20, 14, 32, 22, 38, 43])\n",
    "\n",
    "# The input and output arrays are created but not done yet\n",
    "# Add the column of ones to the onputs if you want statsmodels to calculate the intercept.\n",
    "x = sm.add_constant(x)\n",
    "# We do this because statsmodels doesnt automatically include the intercepy\n",
    "# What x is now:\n",
    "# array([[ 1.,  0.,  1.],\n",
    "#        [ 1.,  5.,  1.],\n",
    "#        [ 1., 15.,  2.],\n",
    "#        [ 1., 25.,  5.],\n",
    "#        [ 1., 35., 11.],\n",
    "#        [ 1., 45., 15.],\n",
    "#        [ 1., 55., 34.],\n",
    "#        [ 1., 60., 35.]])\n",
    "# You can see that the modifies x has three columns:\n",
    "# the first column of ones, corresponding to b0 and replacing the intercept\n",
    "# as well as two columns of the original features.\n",
    "\n",
    "# STEP 3: Create a model and fit it\n",
    "# The regression model based on OLS squares is an instance of the class:\n",
    "# statsmodels.regression.linear_model.OLS\n",
    "# This is how you can obtain one\n",
    "model = sm.OLS(y, x) # NOTICE first argument is output followed by the input\n",
    "\n",
    "# Now that the model is created, you can apply .fit() on it:\n",
    "results = model.fit()\n",
    "\"\"\"\n",
    "By calling .fit(), you obtain the variable results, which is an instance of the class \n",
    "statsmodels.regression.linear_model.RegressionResultsWrapper. \n",
    "    - This object holds a lot of information about the regression model.\n",
    "\"\"\"\n",
    "\n",
    "# STEP 4: Get results\n",
    "# Use .summary() to get the table with the results of linear regression\n",
    "print(results.summary())\n",
    "\n",
    "\"\"\"\n",
    "This table is very comprehensive. You can find many statistical values associated with\n",
    "linear regression, including r2, b0, b1, and b1.\n",
    "\"\"\"\n",
    "\n",
    "# You can extract any of the values from the table above.\n",
    "\n",
    "print(\"r2 score:\", results.rsquared)\n",
    "\n",
    "print(\"r2 score adjusted:\", results.rsquared_adj)\n",
    "\n",
    "print(\"regression coafficients:\", results.params)\n",
    "\n",
    "\"\"\"\n",
    "1. .rsquared holds r2\n",
    "\n",
    "2. .rsquared_adj represents adjusted r2 - that is, r2 corrected according to the number of\n",
    "   input features.\n",
    "   \n",
    "3. .params refers to the array with b0, b1, and b2\n",
    "\"\"\"\n",
    "\n",
    "# STEP 5: Predict response\n",
    "# use .fittedvalued or .predict() with the input array as the argument\n",
    "\n",
    "# Predicted response using .fittedvalues:\n",
    "print(\"Predicted response:\", results.fittedvalues)\n",
    "\n",
    "# Predicted response using .predict()\n",
    "print(\"Predicted response:\", results.predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f316fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.77760476,  7.18179502,  8.58598528,  9.99017554, 11.3943658 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONTINUATION of previous clock of code\n",
    "\n",
    "# Predicting response with new input features\n",
    "# You can also apply .predict with new data as the argument\n",
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "x_new = sm.add_constant(x_new)\n",
    "x_new\n",
    "\n",
    "y_pred = results.predict(x_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d28dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score 0.8615939258756776\n",
      "Intercept 5.522579275198183\n",
      "Slope [0.44706965 0.25502548]\n",
      "Predicted response: [ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "# Multiple Linear Regression with scikit-learn Tutorial starts now\n",
    "# STEP 1: Import packages and classes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# STEP 2: Provide data (known inputs and outputs)\n",
    "\n",
    "X = np.array([\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "])\n",
    "\n",
    "# X is a two dimesnional array with two columns\n",
    "\n",
    "y = np.array([4, 5, 20, 14, 32, 22, 38, 43])\n",
    "# One dimesnional array\n",
    "\n",
    "# STEP 3: Create a model and fit it\n",
    "# The next step is to create the regression model as an instance of LinearRegression and fit it\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# The result of this statement is the variable model referring o the object type LinearRegression\n",
    "# It represents the regression model fitted with existing data\n",
    "\n",
    "# STEP 4: Get Results\n",
    "r2 = model.score(X, y)\n",
    "print(\"r2 score\", r2)\n",
    "\n",
    "intercept = model.intercept_\n",
    "print(\"Intercept\", intercept)\n",
    "\n",
    "slope = model.coef_\n",
    "print(\"Slope\", slope)\n",
    "\n",
    "# In this example, intercept is 5.52, this is the value of the predicted response when x1 = x2 = 0. \n",
    "# An increase of x1 by 1 yields a rise of the predicted response by 0.45. \n",
    "# Similarly, when x2 grows by 1, the response rises by 0.26.\n",
    "\n",
    "# STEP 5: Predict response\n",
    "y_pred = model.predict(X)\n",
    "print(\"Predicted response:\", y_pred)\n",
    "\n",
    "\"\"\"\n",
    "How the predicted y values are calculated:\n",
    "The model predicts:\n",
    "  y = b + w1*x1 + w2*x2\n",
    "  \n",
    "  - b = intercept (5.52)\n",
    "  - w1 = slope for x1 (0.45)\n",
    "  - w2 = slope for x2 (0.26)\n",
    "\n",
    "Why is the prediction for the first y value 5.77 when X = [0, 1]?\n",
    "  1. First Input: X[0] = [0, 1]\n",
    "    - x1 = 0 and x1 = 1\n",
    "  2. Prediction Calculation:\n",
    "    - y = 5.52 + (0.45 * 0) + (0.26*1)\n",
    "        = 5.52 + 0 + 0.26\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc9809d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "[ 5.77760476  7.18179502  8.58598528  9.99017554 11.3943658 ]\n"
     ]
    }
   ],
   "source": [
    "# CONTINUATION or previous code block\n",
    "# Predict y values for new X inputs/features\n",
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf385ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 5  1]\n",
      " [15  2]\n",
      " [25  5]\n",
      " [35 11]\n",
      " [45 15]\n",
      " [55 34]\n",
      " [60 35]]\n",
      "[ 4  5 20 14 32 22 38 43]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f770a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score: 0.8908516262498563\n",
      "intercept: 21.372321428571453\n",
      "Slope: [-1.32357143  0.02839286]\n",
      "Predicted response: [15.46428571  7.90714286  6.02857143  9.82857143 19.30714286 34.46428571]\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression With scikit-learn Tutorial starts now\n",
    "# STEP 1: Import packages and classes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# STEP 2a: Provide data\n",
    "# Define the inputs and outputs\n",
    "X = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1)) # 2D array, one column, 6 rows\n",
    "y = np.array([15, 11, 2, 8, 25, 32])\n",
    "\n",
    "# X, or the input, is a two-dimensional array\n",
    "\n",
    "# STEP 2b: Transform the input data\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "\"\"\"\n",
    "Explanation of 'transformer = PolynomialFeatures(degree=2, include_bias=False)'\n",
    "\n",
    "The variable 'transformer' refers to an instance of PolynomailFeatures that you can use to\n",
    "transform the input X.\n",
    "\n",
    "You can provide several optional parameters to PolynomialFeatures:\n",
    "    - degree: is an integer (2 by default) that represents the degree of the polynomial\n",
    "      regression function.\n",
    "    - interaction_only: is Boolean (False by default) that decides whether to include only\n",
    "      interaction features (True) or all features (False).\n",
    "    - include_bias: is a Boolean (True by default) that decides whether to include the bias,\n",
    "      or interceptm column of 1 values (True) or not (False).\n",
    "      \n",
    "This example uses the default values of all parameters except include_bias.\n",
    "You'll sometimes want to experiement with the degree of the function, and it can be beneficial\n",
    "for readability to provide this argument anayway.\n",
    "\"\"\"\n",
    "\n",
    "# Before applying transformer, you need to fit it with .fit():\n",
    "transformer.fit(X)\n",
    "\n",
    "# Once transformer is fitted, then it's ready to create a new, modified input array\n",
    "# Apply .transform() to do that\n",
    "x_ = transformer.transform(X)\n",
    "\n",
    "# transform() takes the input array as the argument and returns the modified array.\n",
    "# Use .fit_transform() to replace the three previous statements with only one:\n",
    "\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n",
    "\n",
    "\"\"\"\n",
    "With .fit_transform(), you're fitting and transforming the input array in one statement.\n",
    "This method also takes the inout array and effectively does the same thing as .fit() and\n",
    ".transform() called in that order.\n",
    "It also returns the modifies array.\n",
    "\n",
    "This is how the new input array looks:\n",
    "[[   5.   25.]\n",
    " [  15.  225.]\n",
    " [  25.  625.]\n",
    " [  35. 1225.]\n",
    " [  45. 2025.]\n",
    " [  55. 3025.]]\n",
    " \n",
    "The modified array ocntains two columns: one with the original inputs and the other with\n",
    "their squares.\n",
    "\"\"\"\n",
    "\n",
    "# STEP 3: Create a model and fit it\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# The regression model is now created and fitted.\n",
    "# It's ready for application.\n",
    "# Keep in mind that that the first argument of .fit() is the modified array x_ and not original x\n",
    "\n",
    "# STEP 4: Get results\n",
    "# Obtain properties of the model (r2 score, intercept, slope)\n",
    "r2 = model.score(x_, y)\n",
    "print(\"r2 score:\", r2)\n",
    "\n",
    "intercept = model.intercept_\n",
    "print(\"intercept:\", intercept)\n",
    "\n",
    "slope = model.coef_\n",
    "print(\"Slope:\", slope)\n",
    "\n",
    "# Here, ,intercept_ represents b0, while .coef_ references the array that contains b1 and b2.\n",
    "\n",
    "# STEP 5: Predict Response\n",
    "# Use .predict(), but remember that the argument should be the modified input array x_\n",
    "y_pred = model.predict(x_)\n",
    "print(\"Predicted response:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf350777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.9453701449127823\n",
      "intercept: 0.8430556452397582\n",
      "coefficients:\n",
      "[ 2.44828275  0.16160353 -0.15259677  0.47928683 -0.4641851 ]\n",
      "predicted response:\n",
      "[ 0.54047408 11.36340283 16.07809622 15.79139    29.73858619 23.50834636\n",
      " 39.05631386 41.92339046]\n"
     ]
    }
   ],
   "source": [
    "# The same regression as above but with several input variables\n",
    "# Step 1: Import packages and classes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 2a: Provide data\n",
    "x = np.array([\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "])\n",
    "y = np.array([4, 5, 20, 14, 32, 22, 38, 43])\n",
    "\n",
    "# Step 2b: Transform input data\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Step 3: Create a model and fit it\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# Step 4: Get results\n",
    "r2 = model.score(x_, y)\n",
    "intercept, coefficients = model.intercept_, model.coef_\n",
    "\n",
    "# Step 5: Predict response\n",
    "y_pred = model.predict(x_)\n",
    "\n",
    "print(\"coefficient of determination:\", r2)\n",
    "\n",
    "print(\"intercept:\", intercept)\n",
    "\n",
    "print(f\"coefficients:\\n{coefficients}\")\n",
    "\n",
    "print(f\"predicted response:\\n{y_pred}\")\n",
    "\n",
    "\"\"\"\n",
    "In this case there are six regression coefficients, including the intercept, as shown in the\n",
    "estimated regression function: ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ + ùëè‚ÇÉùë•‚ÇÅ¬≤ + ùëè‚ÇÑùë•‚ÇÅùë•‚ÇÇ + ùëè‚ÇÖùë•‚ÇÇ¬≤.\n",
    "\n",
    "You can also notice that polynomial regression yielded a higher coefficient pf determination (r2)\n",
    "than multiple linear regression for the same problem.\n",
    "\n",
    "At first, you could think that obtaining such a large ùëÖ¬≤ is an excellent result. It might be.\n",
    "\n",
    "However, in real-world situations, having a complex model and ùëÖ¬≤ very close to one might also\n",
    "be a sign of overfitting. To check the performance of a model, you should test it with new \n",
    "data‚Äîthat is, with observations not used to fit, or train, the model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b856010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "\n",
      "\n",
      "x_:\n",
      "[[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n",
      "\n",
      "\n",
      "x__:\n",
      "[[ 1.  0.  1.  0.]\n",
      " [ 1.  2.  3.  6.]\n",
      " [ 1.  4.  5. 20.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExplanation:\\n\\nThis code demonstrates how to use scikit-learns PolynomialFeatures to transform input data\\ninto polynomial features.\\n\\n'X = np.arange(6).reshape((3, 2))'\\n\\n    - Creates a 2D array with 3 rows and 2 columns\\n    - Represents 3 data points, each with 2 features\\n    \\n'poly = PolynomialFeatures(2)\\nx_ = poly.fit_transform(X)'\\n\\n    - PolynomialFeatures(2) creates polynomial deatures up to degree 2\\n    \\n'poly = PolynomialFeatures(interaction_only=True)\\nx__ = poly.fit_transform(X)'\\n\\n    - This is the second transformation (interaction only)\\n    - interaction_only=True means we only include interaction terms (products of different features)\\n    - Excludes pure squared terms\\n    \\nOutput Explnation:\\nThe first output (x_) will show:\\n    - A column of 1's (bias term)\\n    - Original features (columns 1-2)\\n    - Squared terms (columns 3-4)\\n    - Interaction term (column 5)\\n\\nThe second output (x__) will show:\\n    - A column of 1's (bias term)\\n    - Original features (columns 1-2)\\n    - Only the interaction term (column 3)\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New PolynomialFeatures Example:\n",
    "# Shows how changin interaction_only works\n",
    "# Transforming an X input to a new modified array\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = np.arange(6).reshape((3, 2))  # 2D array with 2 columns and 3 rows\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "x_ = poly.fit_transform(X)  # New transformed array\n",
    "print(\"x_:\")\n",
    "print(x_)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "x__ = poly.fit_transform(X)\n",
    "print(\"x__:\")\n",
    "print(x__)\n",
    "\n",
    "\"\"\"\n",
    "Explanation:\n",
    "\n",
    "This code demonstrates how to use scikit-learns PolynomialFeatures to transform input data\n",
    "into polynomial features.\n",
    "\n",
    "'X = np.arange(6).reshape((3, 2))'\n",
    "\n",
    "    - Creates a 2D array with 3 rows and 2 columns\n",
    "    - Represents 3 data points, each with 2 features\n",
    "    \n",
    "'poly = PolynomialFeatures(2)\n",
    "x_ = poly.fit_transform(X)'\n",
    "\n",
    "    - PolynomialFeatures(2) creates polynomial deatures up to degree 2\n",
    "    \n",
    "'poly = PolynomialFeatures(interaction_only=True)\n",
    "x__ = poly.fit_transform(X)'\n",
    "\n",
    "    - This is the second transformation (interaction only)\n",
    "    - interaction_only=True means we only include interaction terms (products of different features)\n",
    "    - Excludes pure squared terms\n",
    "    \n",
    "    \n",
    "Output Explnation:\n",
    "For input [a, b], the output becomes:\n",
    "[1, a, b, a¬≤, ab, b¬≤]\n",
    "The first output (x_) will show:\n",
    "    - A column of 1's (bias term, column 0)\n",
    "    - Original features (columns 1-2)\n",
    "    - Squared terms (columns 3 and 5)\n",
    "    - Interaction term (ab, column 4)\n",
    "\n",
    "For input [a, b], the output becomes:\n",
    "[1, a, b, ab] (No squared terms)\n",
    "The second output (x__) will show:\n",
    "    - A column of 1's (bias term)\n",
    "    - Original features (columns 1-2)\n",
    "    - Only the interaction term (column 3)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
